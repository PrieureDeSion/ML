{
  "name": "Machine Learning",
  "tagline": "Andrew NG | Stanford University",
  "body": "I took up the Machine Learning course offered by Andrew NG through [Coursera](https://www.coursera.org/) in the session May 16, 2016 to August 8, 2016. This page is a short guide to the course structure, complete with reviews on assignments, along with the Assignments submitted for evaluation. I always found the idea of making a system ***learn*** from its experiences, like humans, very fascinating. Also, ML was introduced to me as a good way to implement my programming knowledge by putting it to some serious use. My motivation for taking up the course (apart from the fact that it's *cool*) was to get acquainted with scientific computation methods and understand the Math behind \"teaching\" machines how to think! Also, being a Marvel fan, writing my own neural network had always been on my mind ( JARVIS reference).\r\n\r\n***\r\n\r\n## Week 1: Introduction to Machine Learning\r\n\r\nThe first week starts just like you'd expect it to. An introduction to Machine Learning, and it's two broad categories - Supervised and Unsupervised Learning - is given along with common forms of both. The lecturer does no delay in introducing to the real matter of ML, as he starts off by introducing the concept of Linear Regression. The first look at Cost Function and the idea of minimizing the cost turns out to be an interesting one, which helped me make up my mind to continue this course, despite hectic Summers. As a basic method for optimization, Gradient Descent was introduced, along with an intuitive explanation.\r\n***\r\n\r\n## Week 2: Linear Regression with Multiple Variables\r\n<i><b>Motivation</b> : First Assignment this week! Finally some hands-on!</i>  \r\nThe week starts with an introduction to the programming platform meant to be used in the course - **Octave**. If you have a license, you can use MATLAB as well. A majority of the lecture videos are based on basics of operating Octave*(it's kind of different from other languages because the elemental unit of Octave/MATLAB is an array!)* and programming tricks.  Despite being familiar with MATLAB *(used it for Image Processing in the past)* I found the tutorials helpful, especially the one on *vectorization*. The other lecture videos narrate how you can actually implement your first Learning program - how the algorithm goes into code. This week, you will implement Linear regression with Multiple Variables.  \r\n<i><b>Assignment Difficulty</b> : Very Easy!</i>  \r\n***\r\n\r\n## Week 3: Logistic Regression\r\n<i><b>Motivation</b> : Classifiers to be discussed! Most frequently used learning category.</i>  \r\nThis week introduces Classification as a type of ML problem and how Logistic Regression can be employed to make a Multi-class Classifier! The week starts with discussing the decision boundary and the cost function for this kind of problem. Gradient Descent was a simple optimization algorithm, and there are much to advanced and faster ones written already. This week, `fminuc` and `fmincg` are introduced, to optimize the cost function in a more efficient way. Another important topic addressed in this week is the problem of over-fitting of the dataset and how Regularization of Parameters can be implemented to solve this. The week's assignment will have you implement a Multi-variable Classifier using Logistic Regression, with two classes.  \r\n<i><b>Assignment Difficulty</b> : Easy!</i>\r\n***\r\n\r\n## Week 4: Neural Networks - Representation\r\n<i><b>Motivation</b> : Hold your breath... Neural Networks are here! #love </i>  \r\nAnd after the long wait, here comes what most of us were waiting for! The week begins with an idea of how neurons function and how we can model them using what we've learnt so far. The videos give an intuitive feel of how a simple neural network can be used to make a multi-class classifier with a much lower error! The assignment involves a simple Logistic Regression model that helps classify hand-written digits (10 class model). First cool thing you'll be doing. As a taste of neural networks, a part of the assignment involves employing an already trained network for the same process!  \r\n<i><b>Assignment Difficulty</b> : Easy (Same as previous week, with more classes)</i>  \r\n***\r\n\r\n## Week 5: Neural Networks - Learning  \r\n<i><b>Motivation</b> : Write your FIRST Neural Network </i>  \r\nThis week talks real business. The task is clear: write your own neural network (similar to the one provided in the previous week) from scratch! The videos describe how the cost function will be expressed in this problem and how the NN must be designed. For the problem, the course suggests a NN with one hidden layer and the assignment is based on the same model. As a part of optimization, the course discusses the backpropogation algorithm in detail, which is indeed the best math you'll see in this course! The assignment uses all this, and more, to implement your own NN!  \r\n<i><b>Assignment Difficulty</b> : Hard</i>\r\n***\r\n\r\n## Week 6: Advice for Applying Machine Learning  \r\n<i><b>Motivation</b> : Real world application of all you've learnt! Tweaking each Parameter... </i>  \r\nThis is supposed to be the most important set of videos, if you are planning to use ML in designing real systems. The week starts with talking about evaluating the hypothesis and how you can judge the scenario. It also discusses the need for a separate Cross-Validation set and Test set, independent from the Training Set and how each conveys information about the validity of the model. Evaluation is done in terms of 'Bias' and 'Variance' and an integral part of the week is analysis and understanding of the Learning Curves. The assignment involves implementing a Regularized model and plotting Learning curves. You will also be dealing with Cross-Validation curve to analyse the bias-variance conflict.  \r\n<i><b>Assignment Difficulty</b> : Moderate </i>\r\n***\r\n\r\n## Week 7: Support Vector Machines\r\n<i><b>Motivation</b> : NN are old! Time for the new era - SVMs..! </i>  \r\nThis week deals with a whole new kind of approach to Machine Learning. Till now, every feature you've used was user defined. With SVMs you don't need to define the features. The features are defined by each training example! Weird? Read this [blog post](https://prieuredesion.github.io/machine%20learning/2016/06/18/svm-kernel.html) to understand, before starting the week. The videos lectures talk about the much needed math behind the Large Margin Intuition, and is a must watch. It later talks about the *kernels*, which are simple functions that allow each training example to be used as a training feature. The Gaussian and Linear kernels are very well presented. The assignment involves building your own SVM for a simple classification problem firstly, and then for a vocabulary-based spam classifier. Personally, I feel that there's nothing much to do because the `svmTrain.m` function given is a readymade and there is hardly any input from your side. I recommend that you open the file in edit mode and try to understand the function *(for application purposes, a given function is sufficient, but what use is just that if you can do better ;) )*.  \r\n<i><b>Assignment Difficulty</b> : Moderate </i>  \r\n***\r\n\r\n## Week 8: Unsupervised Learning  \r\n<i><b>Motivation</b> : The second type of problems! Also, PCA and compression discussed!! Cool Assignments! </i>  \r\nAll that was done so far was supervised learning, ie, with labelled data. Now the course discusses unsupervised learning methods. The week starts with discussing *Clustering* as the first kind of unsupervised learning. The K-means algorithm is discussed in detail, along with implementation advice. Later on, we start talking about data compression as another important application of unsupervised learning and how dimensionality reduction can be useful in many problems. As a method to achieve dimensionality reduction, the videos introduce Principal Component Analysis (PCA) and why it works *(Linear Algebra is back!)* using the simple concept of eigenvectors. It is suggested to understand this well, no matter how many external sources you need for help. This week's assignment has rather cool examples, with simple implementation of K-Means demonstrates how image compression can be done. Also, PCA has been implemented, with an example of how it can be used to speed up supervised algorithms for face analysis. The course is beginning to mold into a real-world applications based course.  \r\n<i><b>Assignment Difficulty</b> : Easy </i>  \r\n***\r\n## Week 9: Anomaly Detection  \r\n<i><b>Motivation</b> : Understanding a movie recommender system! P.S.: Last Assignment! :D </i>  \r\nAnother application of unsupervised learning. As we saw in the last week too, the course has now moved to real-world applications and we see the algorithm and implementation of a useful system. Gaussian distributions have been used for the implementation. Further, we talk about how and when to choose the anomaly detection algorithm against supervised learning. Later on multivariate Gaussian distribution is introduced as a nice way to establish hidden (dependent) features. In the second part of the course, we look into recommender systems and how we can build one with the Collaborative filtering method. The assignment involves setting up a simple anomaly detection algorithm, and implementing a recommender system based on movie ratings. *Note that this week's assignment will be a great learning experience if you try to implement every function in a vectorized manner. You can actually see the factor by which vectorization can speed up an implementation.*  \r\n<i><b>Assignment Difficulty</b> : Moderate </i>  \r\n***\r\n\r\nI am still pursuing this course and hence will update further sections of this page soon. For nay suggestions/complaints/reviews feel free to ping me any time! Also, have a look at my [website](https://prieuredesion.github.io/)! Thank you so much for visiting.!",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}