{
  "name": "Machine Learning",
  "tagline": "Andrew NG | Stanford University",
  "body": "I took up the Machine Learning course offered by Andrew NG through [Coursera](https://www.coursera.org/) in the session May 16, 2016 to August 8, 2016. This page is a documentation of the course, along with the Assignments submitted for evaluation. I always found the idea of making a system ***learn*** from its experiences, like humans, very fascinating. Also, ML was introduced to me as a good way to implement my programming knowledge by putting it to some serious use. My motivation for taking up the course (apart from the fact that it's *cool*) was to get acquainted with scientific computation methods and understand the Math behind \"teaching\" machines how to think! Also, being a Marvel fan, writing my own neural network had always been on my mind ( JARVIS reference).\r\n\r\n***\r\n\r\n## Week 1: Introduction to Machine Learning\r\n\r\nThe first week starts just like you'd expect it to. An introduction to Machine Learning, and it's two broad categories - Supervised and Unsupervised Learning - is given along with common forms of both. The lecturer does no delay in introducing to the real matter of ML, as he starts off by introducing the concept of Linear Regression. The first look at Cost Function and the idea of minimizing the cost turns out to be an interesting one, which helped me make up my mind to continue this course, despite hectic Summers. As a basic method for optimization, Gradient Descent was introduced, along with an intuitive explanation.\r\n***\r\n\r\n## Week 2: Linear Regression with Multiple Variables\r\n<i><b>Motivation</b> : First Assignment this week! Finally some hands-on!</i>  \r\nThe week starts with an introduction to the programming platform meant to be used in the course - **Octave**. If you have a license, you can use MATLAB as well. A majority of the lecture videos are based on basics of operating Octave*(it's kind of different from other languages because the elemental unit of Octave/MATLAB is an array!)* and programming tricks.  Despite being familiar with MATLAB *(used it for Image Processing in the past)* I found the tutorials helpful, especially the one on *vectorization*. The other lecture videos narrate how you can actually implement your first Learning program - how the algorithm goes into code. This week, you will implement Linear regression with Multiple Variables.  \r\n<i><b>Assignment Difficulty</b> : Very Easy!</i>  \r\n***\r\n\r\n## Week 3: Logistic Regression\r\n<i><b>Motivation</b> : Classifiers to be discussed! Most frequently used learning category.</i>  \r\nThis week introduces Classification as a type of ML problem and how Logistic Regression can be employed to make a Multi-class Classifier! The week starts with discussing the decision boundary and the cost function for this kind of problem. Gradient Descent was a simple optimization algorithm, and there are much to advanced and faster ones written already. This week, `fminuc` and `fmincg` are introduced, to optimize the cost function in a more efficient way. Another important topic addressed in this week is the problem of over-fitting of the dataset and how Regularization of Parameters can be implemented to solve this. The week's assignment will have you implement a Multivariable Classifier using Logistic Regression.  \r\n<i><b>Assignment Difficulty</b> : Easy!</i>\r\n***\r\n\r\n## Week 4: Neural Networks - Representation\r\n<i><b>Motivation</b> : Hold your breath... Neural Networks are here! #love </i>  \r\nAnd after the long wait, here comes what most of us were waiting for!",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}